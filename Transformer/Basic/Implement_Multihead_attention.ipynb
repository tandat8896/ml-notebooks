{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2a0817f5-7a9a-4c47-920b-4224d5342cf6",
      "metadata": {
        "id": "2a0817f5-7a9a-4c47-920b-4224d5342cf6"
      },
      "source": [
        "## Example 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0096daf-3c61-45d8-993f-b79a88e43974",
      "metadata": {
        "id": "f0096daf-3c61-45d8-993f-b79a88e43974",
        "outputId": "be98bd38-7cbf-434d-aad0-a7428b27897c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.0391,  0.0267, -0.0697]]], grad_fn=<TransposeBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the input tensor\n",
        "x = torch.tensor([[[-0.1, 0.1,  0.3]]])\n",
        "\n",
        "# Create the multi-head attention layer\n",
        "layer = nn.MultiheadAttention(embed_dim=3, num_heads=1, bias=False, batch_first=True)\n",
        "\n",
        "custom_weights = torch.tensor( [[-0.3561,  0.3674, -0.5108],\n",
        "                                [ 0.5146, -0.4764, -0.1490],\n",
        "                                [ 0.5072, -0.2932, -0.5633],\n",
        "                                [-0.4932, -0.4468,  0.0736],\n",
        "                                [-0.6879, -0.4689, -0.1026],\n",
        "                                [ 0.1847,  0.1858,  0.4469],\n",
        "                                [-0.4110, -0.4083, -0.5549],\n",
        "                                [ 0.3921, -0.0746, -0.1336],\n",
        "                                [-0.6555, -0.3418, -0.2980]]).float()\n",
        "layer.in_proj_weight = nn.Parameter(custom_weights)\n",
        "\n",
        "custom_out_proj = torch.tensor([[-0.3601,  0.2771, -0.0573],\n",
        "                                [-0.0896,  0.0567, -0.2882],\n",
        "                                [ 0.3200,  0.1517,  0.0580]]).float()\n",
        "layer.out_proj.weight = nn.Parameter(custom_out_proj)\n",
        "\n",
        "# Perform the forward pass\n",
        "# You can use x for both queries, keys, and values in this example\n",
        "output_tensor, attn_output_weights = layer(x, x, x)\n",
        "\n",
        "# Print the shape of the output tensor\n",
        "print(output_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8085ff43-a244-4930-9fea-d9081542c52e",
      "metadata": {
        "id": "8085ff43-a244-4930-9fea-d9081542c52e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d373581-b631-479b-8a6a-45c35c72a303",
      "metadata": {
        "id": "3d373581-b631-479b-8a6a-45c35c72a303",
        "outputId": "3325beca-4a6b-4d8b-c7c8-ad543406b8d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.0391,  0.0267, -0.0697]]], grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the input tensor\n",
        "x = torch.tensor([[[-0.1, 0.1, 0.3]]])\n",
        "\n",
        "q = torch.tensor(  [[-0.3561,  0.3674, -0.5108],\n",
        "                    [ 0.5146, -0.4764, -0.1490],\n",
        "                    [ 0.5072, -0.2932, -0.5633]]).float()\n",
        "k = torch.tensor(  [[-0.4932, -0.4468,  0.0736],\n",
        "                    [-0.6879, -0.4689, -0.1026],\n",
        "                    [ 0.1847,  0.1858,  0.4469]]).float()\n",
        "v = torch.tensor(  [[-0.4110, -0.4083, -0.5549],\n",
        "                    [ 0.3921, -0.0746, -0.1336],\n",
        "                    [-0.6555, -0.3418, -0.2980]]).float()\n",
        "o = torch.tensor([[-0.3601,  0.2771, -0.0573],\n",
        "                  [-0.0896,  0.0567, -0.2882],\n",
        "                  [ 0.3200,  0.1517,  0.0580]]).float()\n",
        "\n",
        "# Define the model parameters\n",
        "embed_dim = 3\n",
        "num_heads = 1\n",
        "head_dim = embed_dim // num_heads\n",
        "\n",
        "# Step 1: Linear projections for queries, keys, and values\n",
        "query_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "key_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "value_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "\n",
        "# Custom weights for linear projections\n",
        "query_proj.weight = nn.Parameter(q)\n",
        "key_proj.weight = nn.Parameter(k)\n",
        "value_proj.weight = nn.Parameter(v)\n",
        "\n",
        "# Step 2: Split the input into multiple heads\n",
        "query = query_proj(x)\n",
        "key = key_proj(x)\n",
        "value = value_proj(x)\n",
        "\n",
        "# Reshape query, key, and value to have shape (batch_size, num_heads, seq_len, head_dim)\n",
        "query = query.view(1, num_heads, -1, head_dim)\n",
        "key = key.view(1, num_heads, -1, head_dim)\n",
        "value = value.view(1, num_heads, -1, head_dim)\n",
        "\n",
        "# Step 3: Compute scaled dot-product attention\n",
        "attention_scores = torch.matmul(query, key.transpose(-2, -1)) / (head_dim ** 0.5)\n",
        "attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "context = torch.matmul(attention_weights, value)\n",
        "\n",
        "# Step 4: Concatenate and project back\n",
        "context = context.view(1, -1, embed_dim)\n",
        "out_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "out_proj.weight = nn.Parameter(o)\n",
        "output = out_proj(context)\n",
        "\n",
        "# Print the shape of the output tensor\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ae3607a-fd1a-4b65-ae1e-96d9d4d37d87",
      "metadata": {
        "id": "2ae3607a-fd1a-4b65-ae1e-96d9d4d37d87"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cbec97a-fad6-4fd4-9579-56bf331fd389",
      "metadata": {
        "id": "4cbec97a-fad6-4fd4-9579-56bf331fd389",
        "outputId": "0e24b652-7efd-4952-e9dc-421e631ca85a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "attention_scores tensor([[[-0.0198]]])\n",
            "attention_weights tensor([[[1.]]])\n",
            "context tensor([[[-0.1662, -0.0868, -0.0580]]])\n",
            "tensor([[ 0.0391,  0.0267, -0.0697]])\n"
          ]
        }
      ],
      "source": [
        "# remove bs\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the input tensor (column vectors)\n",
        "x = torch.tensor([[[-0.1, 0.1, 0.3]]])\n",
        "x = x.reshape(1, 3)\n",
        "\n",
        "q = torch.tensor(  [[-0.3561,  0.3674, -0.5108],\n",
        "                    [ 0.5146, -0.4764, -0.1490],\n",
        "                    [ 0.5072, -0.2932, -0.5633]]).float()\n",
        "k = torch.tensor(  [[-0.4932, -0.4468,  0.0736],\n",
        "                    [-0.6879, -0.4689, -0.1026],\n",
        "                    [ 0.1847,  0.1858,  0.4469]]).float()\n",
        "v = torch.tensor(  [[-0.4110, -0.4083, -0.5549],\n",
        "                    [ 0.3921, -0.0746, -0.1336],\n",
        "                    [-0.6555, -0.3418, -0.2980]]).float()\n",
        "o = torch.tensor([[-0.3601,  0.2771, -0.0573],\n",
        "                  [-0.0896,  0.0567, -0.2882],\n",
        "                  [ 0.3200,  0.1517,  0.0580]]).float()\n",
        "\n",
        "# Define the model parameters\n",
        "embed_dim = 3\n",
        "num_heads = 1\n",
        "head_dim = embed_dim // num_heads\n",
        "\n",
        "# Step 1: Linear projections for queries, keys, and values\n",
        "query = x@q.T\n",
        "key = x@k.T\n",
        "value = x@v.T\n",
        "\n",
        "# Reshape query, key, and value to have shape (batch_size, num_heads, seq_len, head_dim)\n",
        "query = query.view(num_heads, -1, head_dim)\n",
        "key = key.view(num_heads, -1, head_dim)\n",
        "value = value.view(num_heads, -1, head_dim)\n",
        "\n",
        "# Step 3: Compute scaled dot-product attention\n",
        "attention_scores = torch.matmul(query, key.transpose(-2, -1)) / (head_dim ** 0.5)\n",
        "attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "context = torch.matmul(attention_weights, value)\n",
        "\n",
        "\n",
        "print('attention_scores', attention_scores)\n",
        "print('attention_weights', attention_weights)\n",
        "print('context', context)\n",
        "\n",
        "\n",
        "# Step 4: Concatenate and project back\n",
        "context = context.view(-1, embed_dim)\n",
        "output = context@o.T\n",
        "\n",
        "# Print the shape of the output tensor\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3028a337-ebc4-4801-bd99-3c90c5fc5a25",
      "metadata": {
        "id": "3028a337-ebc4-4801-bd99-3c90c5fc5a25"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "fcb30174-5ee3-4bbb-a8f6-e908b830d308",
      "metadata": {
        "id": "fcb30174-5ee3-4bbb-a8f6-e908b830d308"
      },
      "source": [
        "## Example 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58ab7852-0cc5-48ee-9050-d3a64fe609a6",
      "metadata": {
        "id": "58ab7852-0cc5-48ee-9050-d3a64fe609a6",
        "outputId": "4678d2f8-03a9-44a7-8e21-b6196c6065b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[-0.1469, -0.1176,  0.2046],\n",
            "         [-0.1481, -0.1185,  0.2045]]], grad_fn=<TransposeBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the input tensor\n",
        "#x = torch.randn(1, 2, 3)\n",
        "x = torch.tensor([[[-0.1767, -0.2996, -0.6140],\n",
        "                   [ 0.4852, -1.1095, -0.3858]]])\n",
        "\n",
        "# Create the multi-head attention layer\n",
        "layer = nn.MultiheadAttention(embed_dim=3, num_heads=1, bias=False, batch_first=True)\n",
        "\n",
        "custom_weights = torch.tensor( [[-0.3561,  0.3674, -0.5108],\n",
        "                                [ 0.5146, -0.4764, -0.1490],\n",
        "                                [ 0.5072, -0.2932, -0.5633],\n",
        "                                [-0.4932, -0.4468,  0.0736],\n",
        "                                [-0.6879, -0.4689, -0.1026],\n",
        "                                [ 0.1847,  0.1858,  0.4469],\n",
        "                                [-0.4110, -0.4083, -0.5549],\n",
        "                                [ 0.3921, -0.0746, -0.1336],\n",
        "                                [-0.6555, -0.3418, -0.2980]]).float()\n",
        "layer.in_proj_weight = nn.Parameter(custom_weights)\n",
        "\n",
        "custom_out_proj = torch.tensor([[-0.3601,  0.2771, -0.0573],\n",
        "                                [-0.0896,  0.0567, -0.2882],\n",
        "                                [ 0.3200,  0.1517,  0.0580]]).float()\n",
        "layer.out_proj.weight = nn.Parameter(custom_out_proj)\n",
        "\n",
        "# Perform the forward pass\n",
        "# You can use x for both queries, keys, and values in this example\n",
        "output_tensor, attn_output_weights = layer(x, x, x)\n",
        "\n",
        "# Print the shape of the output tensor\n",
        "print(output_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1a83dc6-0238-49a5-a90a-88dcee2af372",
      "metadata": {
        "id": "c1a83dc6-0238-49a5-a90a-88dcee2af372"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f76bde34-4de3-4b05-b6c4-a46716fc5f2b",
      "metadata": {
        "id": "f76bde34-4de3-4b05-b6c4-a46716fc5f2b",
        "outputId": "b34b2bf4-5511-4ca9-ee4a-72f2836a5c8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 2, 3])\n",
            "tensor([[[-0.1469, -0.1176,  0.2046],\n",
            "         [-0.1481, -0.1185,  0.2045]]], grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the input tensor\n",
        "x = torch.tensor([[[-0.1767, -0.2996, -0.6140],\n",
        "                   [ 0.4852, -1.1095, -0.3858]]]) #(1,2,3)\n",
        "\n",
        "q = torch.tensor(  [[-0.3561,  0.3674, -0.5108],\n",
        "                    [ 0.5146, -0.4764, -0.1490],\n",
        "                    [ 0.5072, -0.2932, -0.5633]]).float()\n",
        "k = torch.tensor(  [[-0.4932, -0.4468,  0.0736],\n",
        "                    [-0.6879, -0.4689, -0.1026],\n",
        "                    [ 0.1847,  0.1858,  0.4469]]).float()\n",
        "v = torch.tensor(  [[-0.4110, -0.4083, -0.5549],\n",
        "                    [ 0.3921, -0.0746, -0.1336],\n",
        "                    [-0.6555, -0.3418, -0.2980]]).float()\n",
        "o = torch.tensor([[-0.3601,  0.2771, -0.0573],\n",
        "                  [-0.0896,  0.0567, -0.2882],\n",
        "                  [ 0.3200,  0.1517,  0.0580]]).float()\n",
        "\n",
        "# Define the model parameters\n",
        "embed_dim = 3\n",
        "num_heads = 1\n",
        "head_dim = embed_dim // num_heads\n",
        "\n",
        "# Step 1: Linear projections for queries, keys, and values\n",
        "query_proj = nn.Linear(embed_dim, embed_dim, bias=False) #(3,3)\n",
        "key_proj = nn.Linear(embed_dim, embed_dim, bias=False) #(3,3)\n",
        "value_proj = nn.Linear(embed_dim, embed_dim, bias=False) #(3,3)\n",
        "\n",
        "# Custom weights for linear projections\n",
        "query_proj.weight = nn.Parameter(q) #đỗ weight vào 3,3\n",
        "key_proj.weight = nn.Parameter(k)#đỗ weight vào 3,3\n",
        "value_proj.weight = nn.Parameter(v)#đỗ weight vào 3,3\n",
        "\n",
        "# Step 2: Split the input into multiple heads\n",
        "query = query_proj(x) #(2,3)@(3,3) -> (2,3)\n",
        "key = key_proj(x) #(2,3)@(3,3) -> (2,3)\n",
        "value = value_proj(x) #(2,3)@(3,3) -> (2,3)\n",
        "\n",
        "# Reshape query, key, and value to have shape (batch_size, num_heads, seq_len, head_dim)\n",
        "query = query.view(1, num_heads, -1, head_dim) #(1, 1, 2, 3)\n",
        "key = key.view(1, num_heads, -1, head_dim) #(1, 1, 2, 3)\n",
        "value = value.view(1, num_heads, -1, head_dim)#(1, 1, 2, 3)\n",
        "\n",
        "# Step 3: Compute scaled dot-product attention\n",
        "attention_scores = torch.matmul(query, key.transpose(-2, -1)) / (head_dim ** 0.5) #(1,1,2,2)\n",
        "attention_weights = F.softmax(attention_scores, dim=-1) #(1,1,2,2)\n",
        "context = torch.matmul(attention_weights, value) #(1,1, 2,2) @(1,1,2,3) -> (1,1,2,3)\n",
        "\n",
        "# Step 4: Concatenate and project back\n",
        "context = context.view(1, -1, embed_dim) #(1, 2, 3)\n",
        "out_proj = nn.Linear(embed_dim, embed_dim, bias=False) #(1,2,3)\n",
        "out_proj.weight = nn.Parameter(o) #(1,2,3) -> (1,3,3)\n",
        "output = out_proj(context) #(1,2,3)\n",
        "\n",
        "# Print the shape of the output tensor\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the input tensor\n",
        "x = torch.tensor([[[-0.1, 0.1, 0.3]]])\n",
        "\n",
        "q = torch.tensor(  [[-0.3561,  0.3674, -0.5108],\n",
        "                    [ 0.5146, -0.4764, -0.1490],\n",
        "                    [ 0.5072, -0.2932, -0.5633]]).float()\n",
        "k = torch.tensor(  [[-0.4932, -0.4468,  0.0736],\n",
        "                    [-0.6879, -0.4689, -0.1026],\n",
        "                    [ 0.1847,  0.1858,  0.4469]]).float()\n",
        "v = torch.tensor(  [[-0.4110, -0.4083, -0.5549],\n",
        "                    [ 0.3921, -0.0746, -0.1336],\n",
        "                    [-0.6555, -0.3418, -0.2980]]).float()\n",
        "o = torch.tensor([[-0.3601,  0.2771, -0.0573],\n",
        "                  [-0.0896,  0.0567, -0.2882],\n",
        "                  [ 0.3200,  0.1517,  0.0580]]).float()\n",
        "\n",
        "# Define the model parameters\n",
        "embed_dim = 3\n",
        "num_heads = 1\n",
        "head_dim = embed_dim // num_heads\n",
        "\n",
        "# Step 1: Linear projections for queries, keys, and values\n",
        "query_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "key_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "value_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "\n",
        "# Custom weights for linear projections\n",
        "query_proj.weight = nn.Parameter(q)\n",
        "key_proj.weight = nn.Parameter(k)\n",
        "value_proj.weight = nn.Parameter(v)\n",
        "\n",
        "# Step 2: Split the input into multiple heads\n",
        "query = query_proj(x)\n",
        "key = key_proj(x)\n",
        "value = value_proj(x)\n",
        "\n",
        "# Reshape query, key, and value to have shape (batch_size, num_heads, seq_len, head_dim)\n",
        "query = query.view(1, num_heads, -1, head_dim)\n",
        "key = key.view(1, num_heads, -1, head_dim)\n",
        "value = value.view(1, num_heads, -1, head_dim)\n",
        "\n",
        "# Step 3: Compute scaled dot-product attention\n",
        "attention_scores = torch.matmul(query, key.transpose(-2, -1)) / (head_dim ** 0.5)\n",
        "attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "context = torch.matmul(attention_weights, value)\n",
        "\n",
        "# Step 4: Concatenate and project back\n",
        "context = context.view(1, -1, embed_dim)\n",
        "out_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "out_proj.weight = nn.Parameter(o)\n",
        "output = out_proj(context)\n",
        "\n",
        "# Print the shape of the output tensor\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "CmNLm60FQEy3"
      },
      "id": "CmNLm60FQEy3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ede07953-b37e-47ee-a04e-b0b00cbadd68",
      "metadata": {
        "id": "ede07953-b37e-47ee-a04e-b0b00cbadd68"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4940c7f0-3798-496c-a808-077e532a6b87",
      "metadata": {
        "id": "4940c7f0-3798-496c-a808-077e532a6b87",
        "outputId": "4a34a784-aa21-4865-efc6-0aaaa3951125"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "attention_scores tensor([[[-0.0198,  0.0028],\n",
            "         [ 0.0438, -0.0465]]])\n",
            "attention_weights tensor([[[0.4944, 0.5056],\n",
            "         [0.5225, 0.4775]]])\n",
            "context tensor([[[0.1460, 0.0982, 0.0741],\n",
            "         [0.1286, 0.0879, 0.0667]]])\n",
            "tensor([[-0.0296, -0.0289,  0.0659],\n",
            "        [-0.0258, -0.0258,  0.0583]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define the input tensor\n",
        "x = torch.tensor([[[-0.1, 0.1,  0.3],\n",
        "                   [ 0.4, -1.1, -0.3]]])\n",
        "x = x.reshape(2, 3)\n",
        "\n",
        "q = torch.tensor(  [[-0.3561,  0.3674, -0.5108],\n",
        "                    [ 0.5146, -0.4764, -0.1490],\n",
        "                    [ 0.5072, -0.2932, -0.5633]]).float()\n",
        "k = torch.tensor(  [[-0.4932, -0.4468,  0.0736],\n",
        "                    [-0.6879, -0.4689, -0.1026],\n",
        "                    [ 0.1847,  0.1858,  0.4469]]).float()\n",
        "v = torch.tensor(  [[-0.4110, -0.4083, -0.5549],\n",
        "                    [ 0.3921, -0.0746, -0.1336],\n",
        "                    [-0.6555, -0.3418, -0.2980]]).float()\n",
        "o = torch.tensor([[-0.3601,  0.2771, -0.0573],\n",
        "                  [-0.0896,  0.0567, -0.2882],\n",
        "                  [ 0.3200,  0.1517,  0.0580]]).float()\n",
        "\n",
        "# Define the model parameters\n",
        "embed_dim = 3\n",
        "num_heads = 1\n",
        "head_dim = embed_dim // num_heads\n",
        "\n",
        "# Step 1: Linear projections for queries, keys, and values\n",
        "query = x@q.T\n",
        "key = x@k.T\n",
        "value = x@v.T\n",
        "\n",
        "# Reshape query, key, and value to have shape (batch_size, num_heads, seq_len, head_dim)\n",
        "query = query.view(num_heads, -1, head_dim)\n",
        "key = key.view(num_heads, -1, head_dim)\n",
        "value = value.view(num_heads, -1, head_dim)\n",
        "\n",
        "# Step 3: Compute scaled dot-product attention\n",
        "attention_scores = torch.matmul(query, key.transpose(-2, -1)) / (head_dim ** 0.5)\n",
        "attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "context = torch.matmul(attention_weights, value)\n",
        "\n",
        "\n",
        "print('attention_scores', attention_scores)\n",
        "print('attention_weights', attention_weights)\n",
        "print('context', context)\n",
        "\n",
        "\n",
        "# Step 4: Concatenate and project back\n",
        "context = context.view(-1, embed_dim)\n",
        "output = context@o.T\n",
        "\n",
        "# Print the shape of the output tensor\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1c75211-2a09-4597-8bef-7dc671e4962f",
      "metadata": {
        "id": "a1c75211-2a09-4597-8bef-7dc671e4962f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}