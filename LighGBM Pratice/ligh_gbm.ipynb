{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd9d8623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Tuple, Dict, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a240deb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SplitInfo:\n",
    "    \"\"\"Information about a split decision\"\"\"\n",
    "    feature: int\n",
    "    threshold: float\n",
    "    gain: float\n",
    "    left_indices: np.ndarray\n",
    "    right_indices: np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd955e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistogramBuilder:\n",
    "    \"\"\"Single Responsibility: Build histograms for gradient boosting\"\"\"\n",
    "    \n",
    "    def __init__(self, max_bins: int = 255):\n",
    "        self.max_bins = max_bins\n",
    "        self.bin_mappers = {}\n",
    "    \n",
    "    def build_bin_mappers(self, X: np.ndarray) -> None:\n",
    "        \"\"\"Create bin mappers for each feature\"\"\"\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        for feature_idx in range(n_features):\n",
    "            feature_values = X[:, feature_idx]\n",
    "            unique_values = np.unique(feature_values)\n",
    "            \n",
    "            if len(unique_values) <= self.max_bins:\n",
    "                thresholds = unique_values[:-1] + np.diff(unique_values) / 2\n",
    "            else:\n",
    "                # Use quantiles for binning\n",
    "                quantiles = np.linspace(0, 1, self.max_bins + 1)[1:-1]\n",
    "                thresholds = np.quantile(unique_values, quantiles)\n",
    "                thresholds = np.unique(thresholds)\n",
    "            \n",
    "            self.bin_mappers[feature_idx] = thresholds\n",
    "    \n",
    "    def create_histograms(self, X: np.ndarray, gradients: np.ndarray, \n",
    "                         hessians: np.ndarray, indices: np.ndarray) -> Dict[int, np.ndarray]:\n",
    "        \"\"\"Create histograms for gradient and hessian sums\"\"\"\n",
    "        histograms = {}\n",
    "        \n",
    "        for feature_idx, thresholds in self.bin_mappers.items():\n",
    "            n_bins = len(thresholds) + 1\n",
    "            hist = np.zeros((n_bins, 2))  # [gradient_sum, hessian_sum]\n",
    "            \n",
    "            feature_values = X[indices, feature_idx]\n",
    "            feature_gradients = gradients[indices]\n",
    "            feature_hessians = hessians[indices]\n",
    "            \n",
    "            # Bin the values\n",
    "            bin_indices = np.digitize(feature_values, thresholds)\n",
    "            \n",
    "            # Accumulate gradients and hessians\n",
    "            for i in range(len(indices)):\n",
    "                bin_idx = bin_indices[i]\n",
    "                hist[bin_idx, 0] += feature_gradients[i]\n",
    "                hist[bin_idx, 1] += feature_hessians[i]\n",
    "            \n",
    "            histograms[feature_idx] = hist\n",
    "        \n",
    "        return histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df7ce63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GOSSampler:\n",
    "    \"\"\"Gradient-based One-Side Sampling implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, top_rate: float = 0.2, other_rate: float = 0.1):\n",
    "        self.top_rate = top_rate\n",
    "        self.other_rate = other_rate\n",
    "    \n",
    "    def sample(self, gradients: np.ndarray, hessians: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Apply GOSS sampling strategy\"\"\"\n",
    "        n_samples = len(gradients)\n",
    "        abs_gradients = np.abs(gradients)\n",
    "        \n",
    "        # Get top samples with large gradients\n",
    "        n_top = int(n_samples * self.top_rate)\n",
    "        top_indices = np.argpartition(abs_gradients, -n_top)[-n_top:]\n",
    "        \n",
    "        # Get random sample from remaining\n",
    "        remaining_indices = np.setdiff1d(np.arange(n_samples), top_indices)\n",
    "        n_other = int(len(remaining_indices) * self.other_rate)\n",
    "        other_indices = np.random.choice(remaining_indices, n_other, replace=False)\n",
    "        \n",
    "        # Combine and apply amplification factor\n",
    "        selected_indices = np.concatenate([top_indices, other_indices])\n",
    "        amplification_factor = (1 - self.top_rate) / self.other_rate\n",
    "        \n",
    "        # Amplify gradients and hessians for other samples\n",
    "        sampled_gradients = gradients[selected_indices].copy()\n",
    "        sampled_hessians = hessians[selected_indices].copy()\n",
    "        \n",
    "        other_mask = np.isin(selected_indices, other_indices)\n",
    "        sampled_gradients[other_mask] *= amplification_factor\n",
    "        sampled_hessians[other_mask] *= amplification_factor\n",
    "        \n",
    "        return selected_indices, (sampled_gradients, sampled_hessians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ca8c717",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureBundler:\n",
    "    \"\"\"Exclusive Feature Bundling implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, max_conflict_rate: float = 0.0):\n",
    "        self.max_conflict_rate = max_conflict_rate\n",
    "        self.bundles = []\n",
    "        self.bundle_mapping = {}\n",
    "    \n",
    "    def create_bundles(self, X: np.ndarray) -> None:\n",
    "        \"\"\"Create feature bundles based on sparsity conflicts\"\"\"\n",
    "        n_features = X.shape[1]\n",
    "        conflict_matrix = self._build_conflict_matrix(X)\n",
    "        \n",
    "        # Greedy bundling algorithm\n",
    "        used_features = set()\n",
    "        \n",
    "        for feature in range(n_features):\n",
    "            if feature in used_features:\n",
    "                continue\n",
    "                \n",
    "            bundle = [feature]\n",
    "            used_features.add(feature)\n",
    "            \n",
    "            for other_feature in range(feature + 1, n_features):\n",
    "                if other_feature in used_features:\n",
    "                    continue\n",
    "                    \n",
    "                # Check if feature can be added to bundle\n",
    "                can_bundle = True\n",
    "                for bundled_feature in bundle:\n",
    "                    if conflict_matrix[bundled_feature, other_feature] > self.max_conflict_rate:\n",
    "                        can_bundle = False\n",
    "                        break\n",
    "                \n",
    "                if can_bundle:\n",
    "                    bundle.append(other_feature)\n",
    "                    used_features.add(other_feature)\n",
    "            \n",
    "            self.bundles.append(bundle)\n",
    "            \n",
    "        # Create mapping\n",
    "        for bundle_idx, bundle in enumerate(self.bundles):\n",
    "            for feature in bundle:\n",
    "                self.bundle_mapping[feature] = bundle_idx\n",
    "    \n",
    "    def _build_conflict_matrix(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Build conflict matrix for features\"\"\"\n",
    "        n_features = X.shape[1]\n",
    "        conflict_matrix = np.zeros((n_features, n_features))\n",
    "        \n",
    "        for i in range(n_features):\n",
    "            for j in range(i + 1, n_features):\n",
    "                # Calculate conflict rate (both features non-zero)\n",
    "                conflicts = np.sum((X[:, i] != 0) & (X[:, j] != 0))\n",
    "                total_non_zero = np.sum((X[:, i] != 0) | (X[:, j] != 0))\n",
    "                \n",
    "                if total_non_zero > 0:\n",
    "                    conflict_rate = conflicts / total_non_zero\n",
    "                else:\n",
    "                    conflict_rate = 0\n",
    "                \n",
    "                conflict_matrix[i, j] = conflict_rate\n",
    "                conflict_matrix[j, i] = conflict_rate\n",
    "        \n",
    "        return conflict_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cddd2aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"Tree node implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, indices: np.ndarray, depth: int = 0):\n",
    "        self.indices = indices\n",
    "        self.depth = depth\n",
    "        self.is_leaf = True\n",
    "        self.split_feature = None\n",
    "        self.split_threshold = None\n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "        self.leaf_value = 0.0\n",
    "        self.gain = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab6d7547",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitFinder:\n",
    "    \"\"\"Single Responsibility: Find best splits using histogram method\"\"\"\n",
    "    \n",
    "    def __init__(self, reg_lambda: float = 0.1, reg_gamma: float = 0.0, min_child_samples: int = 20):\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.reg_gamma = reg_gamma\n",
    "        self.min_child_samples = min_child_samples\n",
    "    \n",
    "    def find_best_split(self, histograms: Dict[int, np.ndarray], \n",
    "                       total_gradient: float, total_hessian: float) -> Optional[SplitInfo]:\n",
    "        \"\"\"Find the best split using histogram method\"\"\"\n",
    "        best_gain = 0.0\n",
    "        best_split = None\n",
    "        \n",
    "        for feature_idx, histogram in histograms.items():\n",
    "            split_info = self._find_best_split_for_feature(\n",
    "                feature_idx, histogram, total_gradient, total_hessian\n",
    "            )\n",
    "            \n",
    "            if split_info and split_info.gain > best_gain:\n",
    "                best_gain = split_info.gain\n",
    "                best_split = split_info\n",
    "        \n",
    "        return best_split\n",
    "    \n",
    "    def _find_best_split_for_feature(self, feature_idx: int, histogram: np.ndarray,\n",
    "                                   total_gradient: float, total_hessian: float) -> Optional[SplitInfo]:\n",
    "        \"\"\"Find best split for a single feature\"\"\"\n",
    "        n_bins = histogram.shape[0]\n",
    "        best_gain = 0.0\n",
    "        best_threshold_idx = -1\n",
    "        \n",
    "        left_gradient = 0.0\n",
    "        left_hessian = 0.0\n",
    "        \n",
    "        for threshold_idx in range(n_bins - 1):\n",
    "            left_gradient += histogram[threshold_idx, 0]\n",
    "            left_hessian += histogram[threshold_idx, 1]\n",
    "            \n",
    "            right_gradient = total_gradient - left_gradient\n",
    "            right_hessian = total_hessian - left_hessian\n",
    "            \n",
    "            # Check minimum samples constraint\n",
    "            if left_hessian < self.min_child_samples or right_hessian < self.min_child_samples:\n",
    "                continue\n",
    "            \n",
    "            gain = self._calculate_gain(left_gradient, left_hessian, \n",
    "                                      right_gradient, right_hessian,\n",
    "                                      total_gradient, total_hessian)\n",
    "            \n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_threshold_idx = threshold_idx\n",
    "        \n",
    "        if best_threshold_idx == -1 or best_gain <= self.reg_gamma:\n",
    "            return None\n",
    "        \n",
    "        # Create split info (simplified - would need actual threshold value and indices)\n",
    "        return SplitInfo(\n",
    "            feature=feature_idx,\n",
    "            threshold=best_threshold_idx,  # Simplified\n",
    "            gain=best_gain,\n",
    "            left_indices=np.array([]),  # Would be populated with actual implementation\n",
    "            right_indices=np.array([])  # Would be populated with actual implementation\n",
    "        )\n",
    "    \n",
    "    def _calculate_gain(self, left_grad: float, left_hess: float,\n",
    "                       right_grad: float, right_hess: float,\n",
    "                       total_grad: float, total_hess: float) -> float:\n",
    "        \"\"\"Calculate split gain\"\"\"\n",
    "        left_score = (left_grad ** 2) / (left_hess + self.reg_lambda)\n",
    "        right_score = (right_grad ** 2) / (right_hess + self.reg_lambda)\n",
    "        parent_score = (total_grad ** 2) / (total_hess + self.reg_lambda)\n",
    "        \n",
    "        return 0.5 * (left_score + right_score - parent_score) - self.reg_gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bba02374",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeafWiseTreeBuilder:\n",
    "    \"\"\"Leaf-wise tree growth strategy\"\"\"\n",
    "    \n",
    "    def __init__(self, max_depth: int = 6, max_leaves: int = 31):\n",
    "        self.max_depth = max_depth\n",
    "        self.max_leaves = max_leaves\n",
    "    \n",
    "    def build_tree(self, X: np.ndarray, gradients: np.ndarray, hessians: np.ndarray,\n",
    "                   hist_builder: HistogramBuilder, split_finder: SplitFinder) -> Node:\n",
    "        \"\"\"Build tree using leaf-wise growth\"\"\"\n",
    "        root = Node(np.arange(len(gradients)))\n",
    "        \n",
    "        # Priority queue for leaf-wise growth (max-heap based on gain)\n",
    "        leaf_queue = []\n",
    "        \n",
    "        # Calculate initial leaf value\n",
    "        total_gradient = np.sum(gradients)\n",
    "        total_hessian = np.sum(hessians)\n",
    "        root.leaf_value = -total_gradient / (total_hessian + split_finder.reg_lambda)\n",
    "        \n",
    "        # Try to split root\n",
    "        histograms = hist_builder.create_histograms(X, gradients, hessians, root.indices)\n",
    "        split_info = split_finder.find_best_split(histograms, total_gradient, total_hessian)\n",
    "        \n",
    "        if split_info:\n",
    "            heapq.heappush(leaf_queue, (-split_info.gain, 0, root, split_info))\n",
    "        \n",
    "        n_leaves = 1\n",
    "        \n",
    "        while leaf_queue and n_leaves < self.max_leaves:\n",
    "            neg_gain, _, node, split_info = heapq.heappop(leaf_queue)\n",
    "            \n",
    "            if node.depth >= self.max_depth:\n",
    "                continue\n",
    "            \n",
    "            # Perform split\n",
    "            self._split_node(node, split_info, X)\n",
    "            n_leaves += 1\n",
    "            \n",
    "            # Add new leaves to queue\n",
    "            for child in [node.left_child, node.right_child]:\n",
    "                if child and len(child.indices) > split_finder.min_child_samples:\n",
    "                    child_gradients = gradients[child.indices]\n",
    "                    child_hessians = hessians[child.indices]\n",
    "                    \n",
    "                    child_total_grad = np.sum(child_gradients)\n",
    "                    child_total_hess = np.sum(child_hessians)\n",
    "                    child.leaf_value = -child_total_grad / (child_total_hess + split_finder.reg_lambda)\n",
    "                    \n",
    "                    if child.depth < self.max_depth:\n",
    "                        child_histograms = hist_builder.create_histograms(\n",
    "                            X, gradients, hessians, child.indices\n",
    "                        )\n",
    "                        child_split_info = split_finder.find_best_split(\n",
    "                            child_histograms, child_total_grad, child_total_hess\n",
    "                        )\n",
    "                        \n",
    "                        if child_split_info:\n",
    "                            heapq.heappush(leaf_queue, (-child_split_info.gain, child.depth, child, child_split_info))\n",
    "        \n",
    "        return root\n",
    "    \n",
    "    def _split_node(self, node: Node, split_info: SplitInfo, X: np.ndarray) -> None:\n",
    "        \"\"\"Split a node based on split information\"\"\"\n",
    "        node.is_leaf = False\n",
    "        node.split_feature = split_info.feature\n",
    "        node.split_threshold = split_info.threshold\n",
    "        \n",
    "        # Simplified splitting logic\n",
    "        feature_values = X[node.indices, split_info.feature]\n",
    "        left_mask = feature_values <= split_info.threshold\n",
    "        \n",
    "        left_indices = node.indices[left_mask]\n",
    "        right_indices = node.indices[~left_mask]\n",
    "        \n",
    "        node.left_child = Node(left_indices, node.depth + 1)\n",
    "        node.right_child = Node(right_indices, node.depth + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8b878f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction(ABC):\n",
    "    \"\"\"Interface Segregation: Abstract loss function\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def gradient(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def hessian(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3694d9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquaredLoss(LossFunction):\n",
    "    \"\"\"Squared loss for regression\"\"\"\n",
    "    \n",
    "    def gradient(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        return y_pred - y_true\n",
    "    \n",
    "    def hessian(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        return np.ones_like(y_true)\n",
    "\n",
    "\n",
    "class LogLoss(LossFunction):\n",
    "    \"\"\"Logistic loss for binary classification\"\"\"\n",
    "    \n",
    "    def gradient(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        pred = self._sigmoid(y_pred)\n",
    "        return pred - y_true\n",
    "    \n",
    "    def hessian(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        pred = self._sigmoid(y_pred)\n",
    "        return pred * (1 - pred)\n",
    "    \n",
    "    def _sigmoid(self, x: np.ndarray) -> np.ndarray:\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -250, 250)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701b2431",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fa3562",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
