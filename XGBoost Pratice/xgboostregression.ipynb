{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48504d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e0c8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Our optimized implementation\n",
    "class XGBoostNode:\n",
    "    \"\"\"Optimized XGBoost Node with histogram-based split finding\"\"\"\n",
    "    \n",
    "    def __init__(self, x, gradient, hessian, idxs, \n",
    "                 max_depth=6, min_child_weight=1, reg_lambda=1, \n",
    "                 gamma=0, min_leaf=1, learning_rate=0.3, depth=0,\n",
    "                 max_bins=32):\n",
    "        # Data\n",
    "        self.x = x\n",
    "        self.gradient = gradient\n",
    "        self.hessian = hessian\n",
    "        self.idxs = idxs\n",
    "        self.depth = depth\n",
    "        \n",
    "        # Config parameters\n",
    "        self.max_depth = max_depth\n",
    "        self.min_child_weight = min_child_weight\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.gamma = gamma\n",
    "        self.min_leaf = min_leaf\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_bins = max_bins\n",
    "        \n",
    "        # Node properties\n",
    "        self.n_samples = len(idxs)\n",
    "        self.n_features = x.shape[1]\n",
    "        self.row_count = len(idxs)\n",
    "        \n",
    "        # Split properties\n",
    "        self.score = float('-inf')\n",
    "        self.feature_idx = None\n",
    "        self.split = None\n",
    "        \n",
    "        # Tree structure\n",
    "        self.lhs = None\n",
    "        self.rhs = None\n",
    "        self.val = self.compute_gamma(gradient[idxs], hessian[idxs])\n",
    "        \n",
    "        # Cache for speed\n",
    "        self._histograms = {}\n",
    "        self._bin_edges = {}\n",
    "    \n",
    "    def compute_gamma(self, gradient, hessian):\n",
    "        \"\"\"Compute optimal weight for a leaf node\"\"\"\n",
    "        hessian_sum = np.sum(hessian)\n",
    "        if hessian_sum == 0:\n",
    "            return 0.0\n",
    "        return -np.sum(gradient) / (hessian_sum + self.reg_lambda)\n",
    "    \n",
    "    @property\n",
    "    def is_leaf(self):\n",
    "        \"\"\"Check if node is leaf\"\"\"\n",
    "        return (self.score <= 0 or \n",
    "                self.depth >= self.max_depth or \n",
    "                self.row_count < self.min_leaf or\n",
    "                self.feature_idx is None)\n",
    "    \n",
    "    def create_histograms(self):\n",
    "        \"\"\"Pre-compute histograms for all features - VECTORIZED\"\"\"\n",
    "        if self._histograms:  # Already computed\n",
    "            return\n",
    "            \n",
    "        for feature_idx in range(self.n_features):\n",
    "            x_feature = self.x[self.idxs, feature_idx]\n",
    "            grad_feature = self.gradient[self.idxs]\n",
    "            hess_feature = self.hessian[self.idxs]\n",
    "            \n",
    "            # Create bins using quantiles (weighted by hessian)\n",
    "            if len(np.unique(x_feature)) <= self.max_bins:\n",
    "                bin_edges = np.unique(x_feature)\n",
    "            else:\n",
    "                # Weighted quantiles approximation\n",
    "                sorted_idx = np.argsort(x_feature)\n",
    "                sorted_x = x_feature[sorted_idx]\n",
    "                sorted_hess = hess_feature[sorted_idx]\n",
    "                \n",
    "                # Cumulative hessian weights\n",
    "                cum_hess = np.cumsum(sorted_hess)\n",
    "                total_hess = cum_hess[-1]\n",
    "                \n",
    "                if total_hess > 0:\n",
    "                    # Find quantile positions\n",
    "                    quantile_positions = np.linspace(0, total_hess, self.max_bins + 1)\n",
    "                    bin_edges = np.interp(quantile_positions, cum_hess, sorted_x)\n",
    "                    bin_edges = np.unique(bin_edges)\n",
    "                else:\n",
    "                    bin_edges = np.unique(x_feature)\n",
    "            \n",
    "            self._bin_edges[feature_idx] = bin_edges\n",
    "            \n",
    "            # Build histogram for this feature\n",
    "            if len(bin_edges) > 1:\n",
    "                # Digitize values to bins\n",
    "                bin_indices = np.digitize(x_feature, bin_edges) - 1\n",
    "                bin_indices = np.clip(bin_indices, 0, len(bin_edges) - 2)\n",
    "                \n",
    "                # Aggregate gradients and hessians by bins\n",
    "                hist_grad = np.zeros(len(bin_edges) - 1)\n",
    "                hist_hess = np.zeros(len(bin_edges) - 1)\n",
    "                hist_count = np.zeros(len(bin_edges) - 1)\n",
    "                \n",
    "                # Vectorized aggregation\n",
    "                np.add.at(hist_grad, bin_indices, grad_feature)\n",
    "                np.add.at(hist_hess, bin_indices, hess_feature)\n",
    "                np.add.at(hist_count, bin_indices, 1)\n",
    "                \n",
    "                self._histograms[feature_idx] = {\n",
    "                    'grad': hist_grad,\n",
    "                    'hess': hist_hess,\n",
    "                    'count': hist_count,\n",
    "                    'edges': bin_edges\n",
    "                }\n",
    "    \n",
    "    def find_varsplit_fast(self):\n",
    "        \"\"\"Fast split finding using pre-computed histograms\"\"\"\n",
    "        self.create_histograms()\n",
    "        \n",
    "        for feature_idx in range(self.n_features):\n",
    "            if feature_idx not in self._histograms:\n",
    "                continue\n",
    "                \n",
    "            hist = self._histograms[feature_idx]\n",
    "            self.histogram_search(feature_idx, hist)\n",
    "    \n",
    "    def histogram_search(self, feature_idx, hist):\n",
    "        \"\"\"Search best split using histogram - VECTORIZED\"\"\"\n",
    "        grad_hist = hist['grad']\n",
    "        hess_hist = hist['hess']\n",
    "        count_hist = hist['count']\n",
    "        bin_edges = hist['edges']\n",
    "        \n",
    "        n_bins = len(grad_hist)\n",
    "        if n_bins <= 1:\n",
    "            return\n",
    "        \n",
    "        # Vectorized computation for all possible splits\n",
    "        # Left side: cumulative sum up to each split\n",
    "        left_grad = np.cumsum(grad_hist)[:-1]  # Don't include last bin\n",
    "        left_hess = np.cumsum(hess_hist)[:-1]\n",
    "        left_count = np.cumsum(count_hist)[:-1]\n",
    "        # index 0 → sau bin 1 → left = g1\n",
    "\n",
    "        # index 1 → sau bin 2 → left = g1+g2\n",
    "\n",
    "        # index 2 → sau bin 3 → left = g1+g2+g3\n",
    "        \n",
    "        # Right side: total - left\n",
    "        total_grad = np.sum(grad_hist)\n",
    "        total_hess = np.sum(hess_hist)\n",
    "        total_count = np.sum(count_hist)\n",
    "        \n",
    "        right_grad = total_grad - left_grad\n",
    "        right_hess = total_hess - left_hess  \n",
    "        right_count = total_count - left_count\n",
    "        \n",
    "        # Filter valid splits\n",
    "        valid_mask = ((left_count >= self.min_leaf) & \n",
    "                      (right_count >= self.min_leaf) &\n",
    "                      (left_hess >= self.min_child_weight) &\n",
    "                      (right_hess >= self.min_child_weight) &\n",
    "                      (left_hess > 0) & (right_hess > 0))\n",
    "        \n",
    "        if not np.any(valid_mask):\n",
    "            return\n",
    "        \n",
    "        # Vectorized gain computation\n",
    "        left_score = left_grad**2 / (left_hess + self.reg_lambda)\n",
    "        right_score = right_grad**2 / (right_hess + self.reg_lambda)\n",
    "        parent_score = (total_grad**2) / (total_hess + self.reg_lambda)\n",
    "        \n",
    "        gains = 0.5 * (left_score + right_score - parent_score) - self.gamma\n",
    "        \n",
    "        # Apply valid mask\n",
    "        gains = np.where(valid_mask, gains, float('-inf'))\n",
    "        \n",
    "        # Find best split\n",
    "        best_bin_idx = np.argmax(gains)\n",
    "        best_gain = gains[best_bin_idx]\n",
    "        \n",
    "        if best_gain > self.score and best_gain > 0:\n",
    "            self.score = best_gain\n",
    "            self.feature_idx = feature_idx\n",
    "            # Split at bin edge\n",
    "            self.split = bin_edges[best_bin_idx + 1]\n",
    "    \n",
    "    def build_tree(self):\n",
    "        \"\"\"Build tree with optimizations\"\"\"\n",
    "        # Check stopping criteria\n",
    "        if self.depth >= self.max_depth or self.row_count < self.min_leaf:\n",
    "            return\n",
    "            \n",
    "        # Fast split finding\n",
    "        self.find_varsplit_fast()\n",
    "        \n",
    "        # If no good split found, remain as leaf\n",
    "        if self.score <= 0 or self.feature_idx is None:\n",
    "            return\n",
    "            \n",
    "        # Create child nodes\n",
    "        x_split = self.x[self.idxs, self.feature_idx]\n",
    "        lhs_mask = x_split <= self.split\n",
    "        rhs_mask = x_split > self.split\n",
    "        \n",
    "        lhs_idxs = self.idxs[lhs_mask]\n",
    "        rhs_idxs = self.idxs[rhs_mask]\n",
    "        \n",
    "        # Create children\n",
    "        if len(lhs_idxs) > 0:\n",
    "            self.lhs = XGBoostNode(\n",
    "                self.x, self.gradient, self.hessian, lhs_idxs,\n",
    "                self.max_depth, self.min_child_weight, self.reg_lambda, \n",
    "                self.gamma, self.min_leaf, self.learning_rate, \n",
    "                self.depth + 1, self.max_bins\n",
    "            )\n",
    "            self.lhs.build_tree()\n",
    "            \n",
    "        if len(rhs_idxs) > 0:\n",
    "            self.rhs = XGBoostNode(\n",
    "                self.x, self.gradient, self.hessian, rhs_idxs,\n",
    "                self.max_depth, self.min_child_weight, self.reg_lambda, \n",
    "                self.gamma, self.min_leaf, self.learning_rate, \n",
    "                self.depth + 1, self.max_bins\n",
    "            )\n",
    "            self.rhs.build_tree()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict for multiple samples\"\"\"\n",
    "        return np.array([self.predict_row(xi) for xi in X])\n",
    "    \n",
    "    def predict_row(self, xi):\n",
    "        \"\"\"Predict single sample\"\"\"\n",
    "        if self.is_leaf:\n",
    "            return self.val * self.learning_rate\n",
    "\n",
    "        if self.feature_idx is None:\n",
    "            return self.val * self.learning_rate\n",
    "            \n",
    "        node = self.lhs if xi[self.feature_idx] <= self.split else self.rhs\n",
    "        if node is None:\n",
    "            return self.val * self.learning_rate\n",
    "            \n",
    "        return node.predict_row(xi)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3df57534",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XGBoostRegressor:\n",
    "    \"\"\"Complete XGBoost Regressor implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, n_estimators=100, max_depth=6, learning_rate=0.3,\n",
    "                 min_child_weight=1, reg_lambda=1, gamma=0, min_leaf=1,\n",
    "                 max_bins=32, early_stopping_rounds=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.min_child_weight = min_child_weight\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.gamma = gamma\n",
    "        self.min_leaf = min_leaf\n",
    "        self.max_bins = max_bins\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "        \n",
    "        self.trees = []\n",
    "        self.base_prediction = 0.0\n",
    "    \n",
    "    def _compute_gradient_hessian(self, y_true, y_pred):\n",
    "        \"\"\"Compute gradients and hessians for squared loss\"\"\"\n",
    "        gradient = 2 * (y_pred - y_true)\n",
    "        hessian = 2 * np.ones_like(y_true)\n",
    "        return gradient, hessian\n",
    "    \n",
    "    def fit(self, X, y, eval_set=None, verbose=True):\n",
    "        \"\"\"Fit the XGBoost model\"\"\"\n",
    "        # Initialize base prediction (mean of targets)\n",
    "        self.base_prediction = np.mean(y)\n",
    "        y_pred = np.full(len(y), self.base_prediction, dtype=np.float64)\n",
    "        \n",
    "        # Validation data for early stopping\n",
    "        if eval_set is not None:\n",
    "            X_val, y_val = eval_set\n",
    "            y_val_pred = np.full(len(y_val), self.base_prediction, dtype=np.float64)\n",
    "            best_val_loss = float('inf')\n",
    "            no_improvement_count = 0\n",
    "        \n",
    "        self.trees = []\n",
    "        \n",
    "        for round_num in range(self.n_estimators):\n",
    "            # Compute gradients and hessians\n",
    "            gradient, hessian = self._compute_gradient_hessian(y, y_pred)\n",
    "            \n",
    "            # Fit tree\n",
    "            tree_params = {\n",
    "                'max_depth': self.max_depth,\n",
    "                'min_child_weight': self.min_child_weight,\n",
    "                'reg_lambda': self.reg_lambda,\n",
    "                'gamma': self.gamma,\n",
    "                'min_leaf': self.min_leaf,\n",
    "                'learning_rate': self.learning_rate,\n",
    "                'max_bins': self.max_bins\n",
    "            }\n",
    "            \n",
    "            n_samples = X.shape[0]\n",
    "            idxs = np.arange(n_samples)\n",
    "            \n",
    "            tree = XGBoostNode(X, gradient, hessian, idxs, **tree_params)\n",
    "            tree.build_tree()\n",
    "            \n",
    "            # Update predictions\n",
    "            tree_pred = tree.predict(X)\n",
    "            y_pred += tree_pred\n",
    "            \n",
    "            self.trees.append(tree)\n",
    "            \n",
    "            # Compute training loss\n",
    "            train_loss = mean_squared_error(y, y_pred)\n",
    "            \n",
    "            # Validation and early stopping\n",
    "            if eval_set is not None:\n",
    "                val_tree_pred = tree.predict(X_val)\n",
    "                y_val_pred += val_tree_pred\n",
    "                val_loss = mean_squared_error(y_val, y_val_pred)\n",
    "                \n",
    "                if verbose and round_num % 10 == 0:\n",
    "                    print(f\"Round {round_num:3d}: Train MSE = {train_loss:.6f}, Val MSE = {val_loss:.6f}\")\n",
    "                \n",
    "                # Early stopping logic\n",
    "                if self.early_stopping_rounds:\n",
    "                    if val_loss < best_val_loss:\n",
    "                        best_val_loss = val_loss\n",
    "                        no_improvement_count = 0\n",
    "                    else:\n",
    "                        no_improvement_count += 1\n",
    "                        if no_improvement_count >= self.early_stopping_rounds:\n",
    "                            print(f\"Early stopping at round {round_num}\")\n",
    "                            break\n",
    "            else:\n",
    "                if verbose and round_num % 10 == 0:\n",
    "                    print(f\"Round {round_num:3d}: Train MSE = {train_loss:.6f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict using all fitted trees\"\"\"\n",
    "        predictions = np.full(X.shape[0], self.base_prediction, dtype=np.float64)\n",
    "        \n",
    "        for tree in self.trees:\n",
    "            predictions += tree.predict(X)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ead85402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /home/tandat/miniconda3/envs/xgboost_project/lib/python3.11/site-packages (3.0.5)\n",
      "Requirement already satisfied: numpy in /home/tandat/miniconda3/envs/xgboost_project/lib/python3.11/site-packages (from xgboost) (2.3.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /home/tandat/miniconda3/envs/xgboost_project/lib/python3.11/site-packages (from xgboost) (2.28.3)\n",
      "Requirement already satisfied: scipy in /home/tandat/miniconda3/envs/xgboost_project/lib/python3.11/site-packages (from xgboost) (1.16.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "353c0b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "765c8d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. XGBoostNode\n",
      "Round   0: Train MSE = 1.188244\n",
      "Round  10: Train MSE = 0.555961\n",
      "Round  20: Train MSE = 0.396379\n",
      "Round  30: Train MSE = 0.323572\n",
      "Round  40: Train MSE = 0.291180\n",
      "  Time: 6.49s\n",
      "  MSE:  0.3015\n",
      "  R²:   0.7699\n",
      "\n",
      "2. XGBoost Framework\n",
      "  Time: 0.60s\n",
      "  MSE:  0.3009\n",
      "  R²:   0.7704\n",
      "\n",
      "3. So sánh:\n",
      "  MSE difference: 0.0006\n",
      "  R² difference:  0.0004\n",
      "  Time ratio:     10.73x\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load data\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Model 1: XGBoostNode\n",
    "print(\"\\n1. XGBoostNode\")\n",
    "start_time = time.time()\n",
    "model1 = XGBoostRegressor(n_estimators=50, max_depth=4, learning_rate=0.1, gamma=0.1)\n",
    "model1.fit(X_train_scaled, y_train)\n",
    "time1 = time.time() - start_time\n",
    "\n",
    "pred1 = model1.predict(X_test_scaled)\n",
    "mse1 = mean_squared_error(y_test, pred1)\n",
    "r2_1 = r2_score(y_test, pred1)\n",
    "\n",
    "print(f\"  Time: {time1:.2f}s\")\n",
    "print(f\"  MSE:  {mse1:.4f}\")\n",
    "print(f\"  R²:   {r2_1:.4f}\")\n",
    "\n",
    "# Model 2: XGBoost Framework\n",
    "print(\"\\n2. XGBoost Framework\")\n",
    "start_time = time.time()\n",
    "model2 = xgb.XGBRegressor(n_estimators=50, max_depth=4, learning_rate=0.1, gamma=0.1, random_state=42)\n",
    "model2.fit(X_train_scaled, y_train)\n",
    "time2 = time.time() - start_time\n",
    "\n",
    "pred2 = model2.predict(X_test_scaled)\n",
    "mse2 = mean_squared_error(y_test, pred2)\n",
    "r2_2 = r2_score(y_test, pred2)\n",
    "\n",
    "print(f\"  Time: {time2:.2f}s\")\n",
    "print(f\"  MSE:  {mse2:.4f}\")\n",
    "print(f\"  R²:   {r2_2:.4f}\")\n",
    "\n",
    "# So sánh\n",
    "print(f\"\\n3. So sánh:\")\n",
    "print(f\"  MSE difference: {abs(mse1 - mse2):.4f}\")\n",
    "print(f\"  R² difference:  {abs(r2_1 - r2_2):.4f}\")\n",
    "print(f\"  Time ratio:     {time1/time2:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2be033",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xgboost_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
