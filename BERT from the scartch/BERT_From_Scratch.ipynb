{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BERT Pre-training: MLM + NSP From Scratch\n",
        "\n",
        "Notebook này implement BERT pre-training với **2 objectives kết hợp**:\n",
        "1. **MLM (Masked Language Modeling)**: Dự đoán từ bị mask\n",
        "2. **NSP (Next Sentence Prediction)**: Dự đoán câu tiếp theo có phải là câu kế tiếp không\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Tokenizer đơn giản\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleTokenizer:\n",
        "    def __init__(self, vocab_size=10000):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.word_to_id = {}\n",
        "        self.id_to_word = {}\n",
        "        \n",
        "        # Special tokens\n",
        "        self.special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
        "        self.pad_id, self.unk_id, self.cls_id, self.sep_id, self.mask_id = 0, 1, 2, 3, 4\n",
        "        \n",
        "        for idx, token in enumerate(self.special_tokens):\n",
        "            self.word_to_id[token] = idx\n",
        "            self.id_to_word[idx] = token\n",
        "    \n",
        "    def build_vocab(self, texts):\n",
        "        word_counter = Counter()\n",
        "        for text in texts:\n",
        "            word_counter.update(text.lower().split())\n",
        "        \n",
        "        vocab_count = len(self.special_tokens)\n",
        "        for word, _ in word_counter.most_common():\n",
        "            if vocab_count >= self.vocab_size:\n",
        "                break\n",
        "            if word not in self.word_to_id:\n",
        "                self.word_to_id[word] = vocab_count\n",
        "                self.id_to_word[vocab_count] = word\n",
        "                vocab_count += 1\n",
        "        print(f\"Vocabulary size: {len(self.word_to_id)}\")\n",
        "    \n",
        "    def encode(self, text, max_length=128):\n",
        "        words = text.lower().split()\n",
        "        token_ids = [self.cls_id] + [self.word_to_id.get(w, self.unk_id) for w in words] + [self.sep_id]\n",
        "        \n",
        "        if len(token_ids) > max_length:\n",
        "            token_ids = token_ids[:max_length]\n",
        "            token_ids[-1] = self.sep_id\n",
        "        else:\n",
        "            token_ids.extend([self.pad_id] * (max_length - len(token_ids)))\n",
        "        return token_ids\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. BERT Model Components\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Embedding: Token + Position + Segment\n",
        "class BERTEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, max_seq_length, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_emb = nn.Embedding(max_seq_length, embed_dim)\n",
        "        self.seg_emb = nn.Embedding(2, embed_dim)  # 0: sentence 1, 1: sentence 2\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, token_ids, segment_ids=None):\n",
        "        batch_size, seq_len = token_ids.size()\n",
        "        if segment_ids is None:\n",
        "            segment_ids = torch.zeros_like(token_ids)\n",
        "        \n",
        "        pos = torch.arange(seq_len, device=token_ids.device).unsqueeze(0).expand(batch_size, -1)\n",
        "        emb = self.token_emb(token_ids) + self.pos_emb(pos) + self.seg_emb(segment_ids)\n",
        "        return self.dropout(self.norm(emb))\n",
        "\n",
        "# Multi-Head Attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.scale = math.sqrt(self.head_dim)\n",
        "        \n",
        "        self.q = nn.Linear(embed_dim, embed_dim)\n",
        "        self.k = nn.Linear(embed_dim, embed_dim)\n",
        "        self.v = nn.Linear(embed_dim, embed_dim)\n",
        "        self.out = nn.Linear(embed_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, embed_dim = x.size()\n",
        "        Q = self.q(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = self.k(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.v(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        \n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "        \n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "        context = torch.matmul(attn, V).transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n",
        "        return self.out(context)\n",
        "\n",
        "# Transformer Block\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(ff_dim, embed_dim)\n",
        "        )\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.norm1(x + self.dropout1(self.attn(x, mask)))\n",
        "        x = self.norm2(x + self.dropout2(self.ff(x)))\n",
        "        return x\n",
        "\n",
        "# BERT Encoder\n",
        "class BERT(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=256, num_layers=4, num_heads=8, \n",
        "                 ff_dim=1024, max_seq_length=128, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embeddings = BERTEmbedding(vocab_size, embed_dim, max_seq_length, dropout)\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, ff_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "    \n",
        "    def forward(self, token_ids, segment_ids=None, mask=None):\n",
        "        x = self.embeddings(token_ids, segment_ids)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. BERT Pre-training Model (MLM + NSP)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BERTForPreTraining(nn.Module):\n",
        "    \"\"\"\n",
        "    BERT model cho pre-training với 2 objectives:\n",
        "    - MLM: Dự đoán từ bị mask\n",
        "    - NSP: Dự đoán câu tiếp theo\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embed_dim=256, num_layers=4, num_heads=8,\n",
        "                 ff_dim=1024, max_seq_length=128, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.bert = BERT(vocab_size, embed_dim, num_layers, num_heads, ff_dim, max_seq_length, dropout)\n",
        "        \n",
        "        # MLM Head: Dự đoán từ bị mask\n",
        "        self.mlm_head = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(embed_dim),\n",
        "            nn.Linear(embed_dim, vocab_size)\n",
        "        )\n",
        "        \n",
        "        # NSP Head: Dự đoán câu tiếp theo (IsNext/NotNext)\n",
        "        self.nsp_head = nn.Linear(embed_dim, 2)\n",
        "    \n",
        "    def forward(self, token_ids, segment_ids=None, mask=None):\n",
        "        # BERT encoder output\n",
        "        bert_output = self.bert(token_ids, segment_ids, mask)  # (batch, seq_len, embed_dim)\n",
        "        \n",
        "        # MLM: Dự đoán cho tất cả vị trí\n",
        "        mlm_logits = self.mlm_head(bert_output)  # (batch, seq_len, vocab_size)\n",
        "        \n",
        "        # NSP: Dùng [CLS] token (vị trí 0) để dự đoán\n",
        "        cls_output = bert_output[:, 0, :]  # (batch, embed_dim)\n",
        "        nsp_logits = self.nsp_head(cls_output)  # (batch, 2)\n",
        "        \n",
        "        return mlm_logits, nsp_logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Tạo dữ liệu cho MLM và NSP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_mlm_data(token_ids, tokenizer, mask_prob=0.15):\n",
        "    \"\"\"\n",
        "    Tạo dữ liệu cho MLM:\n",
        "    - 15% tokens được chọn để mask\n",
        "    - Trong số đó: 80% thay bằng [MASK], 10% random token, 10% giữ nguyên\n",
        "    \"\"\"\n",
        "    labels = token_ids.clone()\n",
        "    vocab_size = len(tokenizer.word_to_id)\n",
        "    \n",
        "    for i, token_id in enumerate(token_ids):\n",
        "        # Bỏ qua special tokens\n",
        "        if token_id in [tokenizer.pad_id, tokenizer.cls_id, tokenizer.sep_id]:\n",
        "            labels[i] = -100  # Ignore trong loss\n",
        "            continue\n",
        "        \n",
        "        # 15% chance để mask\n",
        "        if random.random() < mask_prob:\n",
        "            rand = random.random()\n",
        "            if rand < 0.8:  # 80%: thay bằng [MASK]\n",
        "                token_ids[i] = tokenizer.mask_id\n",
        "            elif rand < 0.9:  # 10%: thay bằng random token\n",
        "                token_ids[i] = random.randint(5, vocab_size - 1)\n",
        "            # 10% còn lại: giữ nguyên\n",
        "    \n",
        "    return token_ids, labels\n",
        "\n",
        "def create_nsp_data(sentences, tokenizer, max_length=128):\n",
        "    \"\"\"\n",
        "    Tạo cặp câu cho NSP:\n",
        "    - 50% là câu tiếp theo (is_next=1)\n",
        "    - 50% là câu ngẫu nhiên (is_next=0)\n",
        "    \"\"\"\n",
        "    is_next = random.random() < 0.5\n",
        "    \n",
        "    if is_next:\n",
        "        # Câu tiếp theo: lấy 2 câu liên tiếp\n",
        "        idx = random.randint(0, len(sentences) - 2)\n",
        "        sent1, sent2 = sentences[idx], sentences[idx + 1]\n",
        "    else:\n",
        "        # Không phải câu tiếp theo: lấy 2 câu ngẫu nhiên\n",
        "        idx1, idx2 = random.sample(range(len(sentences)), 2)\n",
        "        sent1, sent2 = sentences[idx1], sentences[idx2]\n",
        "    \n",
        "    # Tokenize: [CLS] sent1 [SEP] sent2 [SEP]\n",
        "    words1 = sent1.lower().split()\n",
        "    words2 = sent2.lower().split()\n",
        "    \n",
        "    token_ids = [tokenizer.cls_id]\n",
        "    segment_ids = [0]\n",
        "    \n",
        "    # Câu 1\n",
        "    for word in words1:\n",
        "        token_ids.append(tokenizer.word_to_id.get(word, tokenizer.unk_id))\n",
        "        segment_ids.append(0)\n",
        "    token_ids.append(tokenizer.sep_id)\n",
        "    segment_ids.append(0)\n",
        "    \n",
        "    # Câu 2\n",
        "    for word in words2:\n",
        "        token_ids.append(tokenizer.word_to_id.get(word, tokenizer.unk_id))\n",
        "        segment_ids.append(1)\n",
        "    token_ids.append(tokenizer.sep_id)\n",
        "    segment_ids.append(1)\n",
        "    \n",
        "    # Pad hoặc truncate\n",
        "    if len(token_ids) > max_length:\n",
        "        token_ids = token_ids[:max_length]\n",
        "        segment_ids = segment_ids[:max_length]\n",
        "        token_ids[-1] = tokenizer.sep_id\n",
        "    else:\n",
        "        padding = max_length - len(token_ids)\n",
        "        token_ids.extend([tokenizer.pad_id] * padding)\n",
        "        segment_ids.extend([0] * padding)\n",
        "    \n",
        "    return token_ids, segment_ids, is_next\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Dataset cho Pre-training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PreTrainingDataset(Dataset):\n",
        "    def __init__(self, sentences, tokenizer, max_length=128):\n",
        "        self.sentences = sentences\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.sentences) - 1\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # Tạo cặp câu cho NSP\n",
        "        token_ids, segment_ids, is_next = create_nsp_data(self.sentences, self.tokenizer, self.max_length)\n",
        "        \n",
        "        # Tạo mask cho valid tokens\n",
        "        mask = [1 if tid != self.tokenizer.pad_id else 0 for tid in token_ids]\n",
        "        \n",
        "        # Áp dụng MLM masking\n",
        "        token_ids_tensor = torch.tensor(token_ids, dtype=torch.long)\n",
        "        token_ids_masked, mlm_labels = create_mlm_data(token_ids_tensor.clone(), self.tokenizer)\n",
        "        \n",
        "        return {\n",
        "            'token_ids': token_ids_masked,\n",
        "            'segment_ids': torch.tensor(segment_ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'mlm_labels': mlm_labels,  # Ground truth cho MLM\n",
        "            'nsp_label': torch.tensor(1 if is_next else 0, dtype=torch.long)  # Ground truth cho NSP\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Loss Function: Kết hợp MLM + NSP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_pretraining_loss(mlm_logits, nsp_logits, mlm_labels, nsp_labels):\n",
        "    \"\"\"\n",
        "    Tính loss kết hợp cho MLM và NSP\n",
        "    \n",
        "    Args:\n",
        "        mlm_logits: (batch, seq_len, vocab_size) - dự đoán từ bị mask\n",
        "        nsp_logits: (batch, 2) - dự đoán IsNext/NotNext\n",
        "        mlm_labels: (batch, seq_len) - ground truth, -100 cho vị trí không tính\n",
        "        nsp_labels: (batch,) - 0 hoặc 1\n",
        "    \"\"\"\n",
        "    # MLM Loss: chỉ tính cho vị trí bị mask (labels != -100)\n",
        "    mlm_loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "    mlm_loss = mlm_loss_fn(\n",
        "        mlm_logits.view(-1, mlm_logits.size(-1)),  # (batch*seq_len, vocab_size)\n",
        "        mlm_labels.view(-1)  # (batch*seq_len,)\n",
        "    )\n",
        "    \n",
        "    # NSP Loss: binary classification\n",
        "    nsp_loss_fn = nn.CrossEntropyLoss()\n",
        "    nsp_loss = nsp_loss_fn(nsp_logits, nsp_labels)\n",
        "    \n",
        "    # Tổng loss: kết hợp cả 2\n",
        "    total_loss = mlm_loss + nsp_loss\n",
        "    \n",
        "    return total_loss, mlm_loss, nsp_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Training: Pre-training với MLM + NSP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 1333\n",
            "Training samples: 1999\n",
            "Model parameters: 3,943,479\n",
            "\n",
            "=== Pre-training với MLM + NSP ===\n",
            "\n",
            "  Batch 20/125: Total=6.9605, MLM=6.2669, NSP=0.6936\n",
            "  Batch 40/125: Total=6.1158, MLM=5.4096, NSP=0.7063\n",
            "  Batch 60/125: Total=5.4566, MLM=4.7008, NSP=0.7558\n",
            "  Batch 80/125: Total=4.9400, MLM=4.2496, NSP=0.6904\n",
            "  Batch 100/125: Total=4.7449, MLM=4.1102, NSP=0.6347\n",
            "  Batch 120/125: Total=4.3986, MLM=3.7173, NSP=0.6813\n",
            "\n",
            "Epoch 1/3:\n",
            "  Total Loss: 5.7477\n",
            "  MLM Loss: 5.0302\n",
            "  NSP Loss: 0.7175\n",
            "  NSP Accuracy: 0.4987\n",
            "\n",
            "  Batch 20/125: Total=4.4226, MLM=3.7060, NSP=0.7166\n",
            "  Batch 40/125: Total=3.8275, MLM=3.0327, NSP=0.7949\n",
            "  Batch 60/125: Total=3.2926, MLM=2.4737, NSP=0.8189\n",
            "  Batch 80/125: Total=3.6135, MLM=2.9332, NSP=0.6803\n",
            "  Batch 100/125: Total=3.8721, MLM=3.1804, NSP=0.6917\n",
            "  Batch 120/125: Total=3.5815, MLM=2.8687, NSP=0.7128\n",
            "\n",
            "Epoch 2/3:\n",
            "  Total Loss: 3.7457\n",
            "  MLM Loss: 3.0369\n",
            "  NSP Loss: 0.7087\n",
            "  NSP Accuracy: 0.4997\n",
            "\n",
            "  Batch 20/125: Total=3.1548, MLM=2.4283, NSP=0.7265\n",
            "  Batch 40/125: Total=2.8395, MLM=2.1434, NSP=0.6961\n",
            "  Batch 60/125: Total=3.0448, MLM=2.3284, NSP=0.7164\n",
            "  Batch 80/125: Total=2.7553, MLM=2.0284, NSP=0.7269\n",
            "  Batch 100/125: Total=2.5711, MLM=1.8554, NSP=0.7157\n",
            "  Batch 120/125: Total=3.2508, MLM=2.5001, NSP=0.7508\n",
            "\n",
            "Epoch 3/3:\n",
            "  Total Loss: 2.9705\n",
            "  MLM Loss: 2.2671\n",
            "  NSP Loss: 0.7034\n",
            "  NSP Accuracy: 0.4947\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "from datasets import load_dataset\n",
        "ds = load_dataset(\"uitnlp/vietnamese_students_feedback\")\n",
        "\n",
        "# Build tokenizer\n",
        "tokenizer = SimpleTokenizer(vocab_size=5000)\n",
        "tokenizer.build_vocab(ds[\"train\"][\"sentence\"][:2000])\n",
        "\n",
        "# Tạo dataset\n",
        "sentences = ds[\"train\"][\"sentence\"][:2000]\n",
        "train_dataset = PreTrainingDataset(sentences, tokenizer, max_length=128)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "\n",
        "# Tạo model\n",
        "vocab_size = len(tokenizer.word_to_id)\n",
        "model = BERTForPreTraining(\n",
        "    vocab_size=vocab_size,\n",
        "    embed_dim=256,\n",
        "    num_layers=4,\n",
        "    num_heads=8,\n",
        "    ff_dim=1024,\n",
        "    max_seq_length=128,\n",
        "    dropout=0.1\n",
        ").to(device)\n",
        "\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "num_epochs = 3\n",
        "\n",
        "# Training loop\n",
        "print(\"\\n=== Pre-training với MLM + NSP ===\\n\")\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_mlm_loss = 0\n",
        "    total_nsp_loss = 0\n",
        "    nsp_correct = 0\n",
        "    nsp_total = 0\n",
        "    \n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        token_ids = batch['token_ids'].to(device)\n",
        "        segment_ids = batch['segment_ids'].to(device)\n",
        "        mask = batch['mask'].to(device)\n",
        "        mlm_labels = batch['mlm_labels'].to(device)\n",
        "        nsp_labels = batch['nsp_label'].to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward: nhận cả MLM và NSP logits\n",
        "        mlm_logits, nsp_logits = model(token_ids, segment_ids, mask)\n",
        "        \n",
        "        # Tính loss kết hợp\n",
        "        loss, mlm_loss, nsp_loss = compute_pretraining_loss(\n",
        "            mlm_logits, nsp_logits, mlm_labels, nsp_labels\n",
        "        )\n",
        "        \n",
        "        # Backward\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Track metrics\n",
        "        total_loss += loss.item()\n",
        "        total_mlm_loss += mlm_loss.item()\n",
        "        total_nsp_loss += nsp_loss.item()\n",
        "        \n",
        "        # NSP accuracy\n",
        "        nsp_preds = nsp_logits.argmax(1)\n",
        "        nsp_correct += (nsp_preds == nsp_labels).sum().item()\n",
        "        nsp_total += nsp_labels.size(0)\n",
        "        \n",
        "        if (batch_idx + 1) % 20 == 0:\n",
        "            print(f\"  Batch {batch_idx+1}/{len(train_loader)}: \"\n",
        "                  f\"Total={loss.item():.4f}, MLM={mlm_loss.item():.4f}, NSP={nsp_loss.item():.4f}\")\n",
        "    \n",
        "    # Epoch summary\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    avg_mlm = total_mlm_loss / len(train_loader)\n",
        "    avg_nsp = total_nsp_loss / len(train_loader)\n",
        "    nsp_acc = nsp_correct / nsp_total\n",
        "    \n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}:\")\n",
        "    print(f\"  Total Loss: {avg_loss:.4f}\")\n",
        "    print(f\"  MLM Loss: {avg_mlm:.4f}\")\n",
        "    print(f\"  NSP Loss: {avg_nsp:.4f}\")\n",
        "    print(f\"  NSP Accuracy: {nsp_acc:.4f}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tóm tắt\n",
        "\n",
        "**BERT Pre-training sử dụng 2 objectives kết hợp:**\n",
        "\n",
        "1. **MLM (Masked Language Modeling)**:\n",
        "   - Mask 15% tokens\n",
        "   - Dự đoán từ bị mask\n",
        "   - Loss: CrossEntropy (chỉ tính cho vị trí bị mask)\n",
        "\n",
        "2. **NSP (Next Sentence Prediction)**:\n",
        "   - Tạo cặp câu: 50% là câu tiếp theo, 50% là câu ngẫu nhiên\n",
        "   - Dự đoán IsNext/NotNext từ [CLS] token\n",
        "   - Loss: CrossEntropy (binary classification)\n",
        "\n",
        "**Total Loss = MLM Loss + NSP Loss**\n",
        "\n",
        "Cả 2 objectives được train **cùng lúc** trong một forward pass, giúp BERT học được:\n",
        "- Hiểu ngữ nghĩa từ (từ MLM)\n",
        "- Hiểu quan hệ giữa các câu (từ NSP)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "torchenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
